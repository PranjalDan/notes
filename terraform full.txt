Terraform complete tutorial:

Terraform is a tool that allows you to create infra automatically.
we need terraform to automate the creation of complete infra.

terraform vs Cloud formation ( aws ka terraform ).
-----------------------------------------------------

AWS makes infrastructure ( resources / services ) where machines are called EC-2 instances, storage is S3 bucket, EKS as Elastic kubernetes services,  and tables etc
similarily azure has VM, AKS, Storage and similary GCS as well 


All these services/resources can be created by terraform
----------------

to install terraform directly type install terraform and search any flavour you want to install..

-----
terraform uses HCL language for coding..

---
HCL mantra
<block> <parameters>
{ 
--
--
---   agruements 
}

---------------
block --- variable block, resource block, output block

parameters --- jo b block banaya hai uska type kya hai that will come as parameter.


e.g -- block -> resource, parameters - aws_instance, { arguments}

-----
terraform workflow:

first file created -- main.tf ( initialise terraform first by terraform init ) 
second we can validate our file by terraform validate
third we do planning by terraform plan where the output from your file is shown like dry run
then at last we have terraform apply for final output.

terraform destroy is finally to destroy the resource, output etc.


example
resource local_file my_file {

filename = automate.txt
content = "here resource is block, parameter is my_local for file creation and this is args"

}

in above example local is a provider and local_file is a type of resource, and both combined, local_file my_file is parameter.
my_file is a resource name -- identifier or we can say refernce name, filename will come on arguements

------------------

creating S3 bucket
here remember that in resource type we wont have aws_s3_bucket provider so do first terraform init.
if u want to install aws custom then directly google terraform providers and copy the code to terraform.tf
for installing the S3 bucket, you should have ur aws account connected to machine which can be done by aws cli installation

#this is S3 bucket, learn from gpt too.

resource aws_s3_bucket my_bucket {
    bucket = "pranjal-ki-bucket"     #name of bucket
}


# if u want to configure provider in some region then create a terraform file provider.tf

provider "aws" {
       region = "us-east-2"
}
   
then terraform plan, terraform apply.

----------------
YOU NEED TO CONFIGURE your AWS account by AWS configure.

for ec2 machine, first have terraform.tf, provider.tf and then ec2 instance example is below:

for creating an ec2 instance, we require Key pair ( login ) and VPC & security group


so first generate a key by ssh-keygen and save it so 

#key pair (login)

resource aws_key_pair {

    key_name = "terra-key-ec2"
    public_key = file("terra-key-ec2.pub")  --- function to directly locate the file instead pasting the key here

}

#VPC & Security group

resource aws_default_vpc default {

}

resource aws_security_group my_security_group {

   name = "automate-sg"
   description = "this will add a TF generated secrurity grp"
   vpc_id = aws_default_vpc.default.id    #interpolation - learn defination of interpolation as it is asked 
   
   tags = 
     Name = "allow_tls"
}
}

#inbound rules - ingress
#outbound rules - egress

example:

resource aws_security_group my_security_group {

   name = "automate-sg"
   description = "this will add a TF generated secrurity grp"
   vpc_id = aws_default_vpc.default.id    #interpolation - learn defination of interpolation as it is asked 
   
# inbound rules (incoming)  learn interpolation too
   ingress {
      from_port =  22
      to_port = 22
      protocol = "tcp"
      cidr_block= [0.0.0.0/0]
      }
   ingress {
      from_port =  80
      to_port = 80
      protocol = "tcp"
      cidr_block= [0.0.0.0/0]
      }


# outbound rules

egress {
      from_port =  0
      to_port = 0
      protocol = "-1"
      cidr_block= [0.0.0.0/0]
      description = "all access open"
      }


   tags = 
     Name = "allow_tls"
}
 
#ec2 instance

resource "aws_instance" "my_instance" {
   key_name = aws_key_pair.my_key.key_name
   security_groups = [aws_security_group.my_security_group.name]
   instance_type = "t2.micro"
   ami = "ferfefee" #ubuntu ami id
   
  root_block_device {
      volume_size = 15
      volume_type = "gp3"
 }
   tags = 
     Name = "Pranjal EC2 instance"

}


now init, validate,plan,apply

u need to give admin rights to terra admin user.

u can now ssh the terraform from local 

-----------

tomorrow like if we want to change any value so instead changing the complete value we can have varibable.tf
where we can have those varibles

variable "aws_instance_type" {
    default = "t2.mjicro"
    type = string
}

etc etc so on other variables

if you want to refer a variable in main.tf it is done like this:

instance_type = var.ec2_instance_type

------------

if you want to see the output -- output block
basically ye isliye hota h ki tumhe output dikh jaae us particular cheez ka
output.tf

output "ec2_public_ip" {

     value = aws_instance.mh_instance.public_ip
}

then plan, apply

-------------------

now if you want to install nginx after you create the instance, so 
firstly deveop the shell script for installing the nginx by any name of shell script.

now simply in terraform file add this line when creating the aws_instance

user_data = file("install_nginx.sh")

this will run this script.
-----------------

3:07 - timestamp for github repo

-----------------

if you want to make multiple resources then first option is to copy paste same code in another block and change the
instance name, second is mostly used option i.e "count" module, jitne count, utne instances

example:

resource "aws_instance" "my_instance" {
   count=2 #it is meta arguement 
   key_name = aws_key_pair.my_key.key_name
   security_groups = [aws_security_group.my_security_group.name]
   instance_type = "t2.micro"
   ami = "ferfefee" #ubuntu ami id
   
  root_block_device {
      volume_size = 15
      volume_type = "gp3"
 }
   tags = 
     Name = "Pranjal EC2 instance"

}


--
in output.tf u just need to give array because now there are multiple instances in place.


output "ec2_public_ip" {

     value = aws_instance.mh_instance[*].public_ip   --- ashtresik is for multiple outputs or range.

}

------------

another meta keyword is for_each -- it uses tomap ({ function.

resource "aws_instance" "my_instance" {
   for_each = tomap({                            #it'll make 2 instances and absed on key value pair  
     "TWS-Junoon-automate-micro" = "t2.micro
     "Prajal 2" = "t2.medium"
})
   key_name = aws_key_pair.my_key.key_name
   security_groups = [aws_security_group.my_security_group.name]
   instance_type = each.value       #### here we add value pair
   ami = "ferfefee" #ubuntu ami id
   
  root_block_device {
      volume_size = 15
      volume_type = "gp3"
 }
   tags = 
     Name = each.key     ###### here we add key pair

}


count was giving same instance name for all but for-each gives different names of all instances.


----------------

here in for_each output is designed in this way:

output "ec2_public_ip" {
   value = [
      for key in aws_instance.my_instance : key.public_ip
     ]

}

----------------
another meta arguement is depends_on

resource "aws_instance" "my_instance" {
   for_each = tomap({                            #it'll make 2 instances and absed on key value pair  
     "TWS-Junoon-automate-micro" = "t2.micro
     "Prajal 2" = "t2.medium"
})

depends_on = [ aws_security_group.my_security_group, aws_key_pair.my_key.key_name] ## jabtak ye nahi ban jaata mera key pair nahi banne wala
   key_name = aws_key_pair.my_key.key_name
   security_groups = [aws_security_group.my_security_group.name]
   instance_type = each.value       #### here we add value pair
   ami = "ferfefee" #ubuntu ami id
   
  root_block_device {
      volume_size = 15
      volume_type = "gp3"
 }
   tags = 
     Name = each.key     ###### here we add key pair

}


---------------------

Conditional expressions:(like if else in other coding lang)

for example make a variable env and make default prod

e.g:

root_block_device {
      volume_size = var.env == "prd" ? 20 : var.ec2.default_root_storage_size   ### : se pehle wala is true
      volume_type = "gp3"
 }

-------------

Terraform state:

tf state is basically a file which maintains the record of how many rresouces u made, when made, their current status etc. everything

if I stop any instance in aws but the state in tfstate file is still running only so that time we need 
to refresh the state

terraform refresh  ##command name



terraform state list  -- it shows all state
terraform state show my_key_pair

If I dont want to maintain the state of anything so simple

terraform state rm my_key_pair

key wont be deleted but state will not be maintatined

if there is somethign existing on aws but not on terraform and u want to bring it on terraform then we use import command.

terraform import aws_key_pair my_key  
-----------
if u want to import a instance which u created manually in aws so this is below thing:

resource "aws_instance" "my_instance_new"
   {
      ami = "unknown"   ##pata ni h
      instance_type = "unknown"

}

terraform import aws_instance.my_instqnace_new i3fwfwff --- instance id

then import is successfull...

then just make terraform state show my_instance

it'll show manual
-------------------------

learn import from documentation too

--------------------------

secure state management:

if the state file is deleted and also we cant commit to github, we cant do this destroy and apply again because we shouldnt do again and again

learn state conflict..

learn state file locking
lock and release mechanism

----------------


state file locking is done using dynamodb, 
if anyone changees in state file then a trigger is generated from s3 bucket to dynamodb and a lock ID is generated.

practically -- 
we need to make s3 bucket, dynamodb, terraform.tf.,providers.tf


------------------

üìù Terraform Modules ‚Äì Easy Notes
1Ô∏è‚É£ What is a Module?

Think of a module as a folder that contains Terraform code (resources, variables, outputs) which you can reuse in multiple places.

Every Terraform configuration (even a single .tf file) is already a module ‚Äî it‚Äôs called the root module.

When you break out code into separate folders and call them from the root, those are child modules.

2Ô∏è‚É£ Why Use Modules?

Reusability: Write once, use for many environments/projects.

Organization: Keep your root folder clean; group related resources.

Consistency: Same infrastructure patterns across environments.

Example: A ‚Äúvpc‚Äù module, an ‚Äúec2‚Äù module, a ‚Äúsecurity group‚Äù module.

3Ô∏è‚É£ Structure of a Module

A module is usually a folder with these files:

vpc-module/
  main.tf       # Actual resources
  variables.tf  # Input variables
  outputs.tf    # Output values


You then call it from your root module:

main.tf (root module)
|
|__ module "vpc" {
      source = "./vpc-module"
      vpc_cidr = "10.0.0.0/16"
   }

4Ô∏è‚É£ How to Call a Module

In the root module:

module "my_vpc" {
  source   = "./vpc-module"   # local path OR remote repo
  vpc_cidr = "10.0.0.0/16"    # pass variables defined in variables.tf
}


The module itself (vpc-module/main.tf) may have:

variable "vpc_cidr" {}

resource "aws_vpc" "main" {
  cidr_block = var.vpc_cidr
}

output "vpc_id" {
  value = aws_vpc.main.id
}


Now back in the root, you can use:

output "my_vpc_id" {
  value = module.my_vpc.vpc_id
}

5Ô∏è‚É£ Module Sources

You can pull modules from:

Local paths: source = "./modules/vpc"

Git repos: source = "git::https://github.com/org/module.git"

Terraform Registry: source = "terraform-aws-modules/vpc/aws"

6Ô∏è‚É£ Variables & Outputs

variables.tf defines inputs the caller must pass.

outputs.tf exposes values back to the caller (like module.vpc.vpc_id).

This allows you to chain modules ‚Äî e.g., output a subnet ID from a VPC module and pass it to an EC2 module.

7Ô∏è‚É£ Best Practices

Name your modules clearly: modules/vpc, modules/ec2.

Always include variables.tf and outputs.tf.

Pin module versions when using registry or git (avoid breaking changes).

Avoid hard-coding values inside the module; use variables.

8Ô∏è‚É£ In Interviews

They might ask:

‚ÄúHow do you structure Terraform code for multiple environments?‚Äù
Answer: ‚ÄúI use modules for each component and call them from environment-specific root modules.‚Äù

‚ÄúHow do you share state between modules?‚Äù
Answer: ‚ÄúBy using outputs from one module and passing them as variables to another.‚Äù

‚ÄúWhere can modules be stored?‚Äù
Answer: ‚ÄúLocally, in private Git repos, or in Terraform Registry.‚Äù


--------------------























